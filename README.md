# RBERT for Relation Extraction task for KLUE

## Hardware

- `GPU : Tesla V100 32GB`

## Project Description

Relation Extraction task is one of [KLUE Benchmark](https://github.com/KLUE-benchmark/KLUE)'s task. 

Korean Language Understanding Evaluation(KLUE) Benchmark is composed of 8 tasks:

- Topic Classification (TC)
- Sentence Textual Similarity (STS)
- Natural Language Inference (NLI)
- Named Entity Recognition (NER)
- **Relation Extraction (RE)**
- (Part-Of-Speech) + Dependency Parsing (DP)
- Machine Reading Comprehension (MRC)
- Dialogue State Tracking (DST)

This repo contains custom dataset, custom training code utilizing [monologg's R-BERT Implementation](https://github.com/monologg/R-BERT).


## Arguments Usage

- RBERT

| Argument               | type  | Default                         | Explanation                                  |
| ---------------------- | ----- | ------------------------------- | -------------------------------------------- |
| batch_size             | int   | 40                              | í•™ìŠµ&ì˜ˆì¸¡ì— ì‚¬ìš©ë  batch size                |
| num_folds              | int   | 5                               | Stratified KFoldì˜ fold ê°œìˆ˜                 |
| num_train_epochs       | int   | 5                               | í•™ìŠµ epoch                                   |
| loss                   | str   | focalloss                       | loss function                                |
| gamma                  | float | 1.0                             | focalloss ì‚¬ìš©ì‹œ gamma ê°’                    |
| optimizer              | str   | adamp                           | í•™ìŠµ optimizer                               |
| scheduler              | str   | get_cosine_schedule_with_warmup | learning rateë¥¼ ì¡°ì ˆí•˜ëŠ” scheduler           |
| learning_rate          | float | 0.00005                         | ì´ˆê¸° learning rate ê°’                        |
| weight_decay           | float | 0.01                            | Loss functionì— Weigthê°€ ì»¤ì§ˆ ê²½ìš° íŒ¨ë„í‹° ê°’ |
| warmup_step            | int   | 500                             |
| debug                  | bool  | false                           | ë””ë²„ê·¸ ëª¨ë“œì¼ ê²½ìš° True                      |
| dropout_rate           | float | 0.1                             | dropout ë¹„ìœ¨                                 |
| save_steps             | int   | 100                             | ëª¨ë¸ ì €ì¥ step ìˆ˜                            |
| evaluation_steps       | int   | 100                             | evaluationí•  step ìˆ˜                         |
| metric_for_best_model  | str   | eval/loss                       | ìµœê³  ì„±ëŠ¥ì„ ê°€ëŠ í•˜ëŠ” metric                  |
| load_best_model_at_end | bool  | True                            |

## References

- [monologg's R-BERT Implementation in Pytorch](https://github.com/monologg/R-BERT)
- [Enriching Pre-trained Language Model with Entity Information for Relation Classification](https://arxiv.org/abs/1905.08284?context=cs)

## Authorship

- [jjonhwa](https://github.com/jjonhwa)
- [ğŸ¤š snoop2head](https://github.com/snoop2head)
- [kimyeondu](kimyeondu)
- [hihellohowareyou](https://github.com/hihellohowareyou)
- [shawnhyeonsoo](https://github.com/shawnhyeonsoo)
- [danielkim30433](https://github.com/danielkim30433)
- [ntommy11](https://github.com/ntommy11)
